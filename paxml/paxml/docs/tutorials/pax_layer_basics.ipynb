{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-vBE5Vpm3jd"
   },
   "source": [
    "# Pax Workshop\n",
    "## Pax Layer Basics\n",
    "\n",
    "Goal: This lab describes the basics for authoring a new Pax layer.\n",
    "\n",
    "*   Pax is built on top of Flax nn.Module\n",
    "*   Familiarity with the basics of Flax will help users understand Pax layer\n",
    "API. See https://flax.readthedocs.io/en/latest/overview.html for Flax basics\n",
    "*   Pax has some of its roots in Lingvo. We try to highlight API differences with Lingvo to help Lingvo users to familiarize with the new API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nRRwV_TVtEo"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from praxis import base_layer\n",
    "from praxis import pax_fiddle\n",
    "from praxis import py_utils\n",
    "from praxis import pytypes\n",
    "from praxis.layers import activations\n",
    "from pprint import pprint\n",
    "\n",
    "# Introduce some common alias.\n",
    "NestedMap = py_utils.NestedMap\n",
    "WeightInit = base_layer.WeightInit\n",
    "WeightHParams = base_layer.WeightHParams\n",
    "LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]\n",
    "template_field = base_layer.template_field\n",
    "instance_field = base_layer.instance_field\n",
    "instantiate = base_layer.instantiate\n",
    "\n",
    "PARAMS = base_layer.PARAMS\n",
    "RANDOM = base_layer.RANDOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egtQL93nu3yW"
   },
   "source": [
    "## Layer definition\n",
    "\n",
    "A 'Pax layer' represents an arbitrary function, possibly with trainable parameters.\n",
    "\n",
    "Layers are the essential building blocks of models. They inherit from the Flax nn.Module. A layer can contain other layers as children.\n",
    "\n",
    "A *Pax layer* always inherits from `base_layer.BaseLayer` (which internally inherits from Flax nn.Module). All non-trivial Pax layers have one or more\n",
    "*fields* and a `__call__` method. Additionally, a `setup` method can be defined to initialize variables and create child layers from templates. A quick preview of what a layer looks like is:\n",
    "```\n",
    "class Linear(base_layer.BaseLayer):\n",
    "  # Hyperparameters:\n",
    "  input_dims: int = 0\n",
    "  ...\n",
    "\n",
    "  def setup(self):\n",
    "    self.create_variable('w', WeightHParams(shape=[self.input_dims, ...\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    return jnp.matmul(inputs, self.theta.w)\n",
    "```\n",
    "\n",
    "Pax layer fields can be divided into three groups:\n",
    "\n",
    "* *Hyperparameters*\n",
    "* *Child layers*\n",
    "* *Layer templates*\n",
    "\n",
    "### PAX Layer Fields: hyperparameters\n",
    "\n",
    "The ***hyperparameters*** for a layer are declared using dataclass field syntax:\n",
    "\n",
    "```\n",
    "  <name>: <type> = <default_value>\n",
    "```\n",
    "\n",
    "All hyperparameters are currently required to have default values.  Hyperparameter values are frozen when a layer is instantiated.\n",
    "\n",
    "### PAX Layer Fields: child layers\n",
    "\n",
    "***Child layers*** can be declared using the following syntax: \n",
    "\n",
    "```\n",
    "<name>: <type> = instance_field(<factory>)\n",
    "````\n",
    "\n",
    "Where `<factory>` is typically a Layer class name (e.g., `Bias`), but can also be a factory function that returns a Layer.\n",
    "\n",
    "Alternatively, child layers can be constructed from layer templates using `self.create_child`, as described below.  Child layers that are constructed from templates should *not* be declared using dataclass field syntax.\n",
    "\n",
    "### PAX Layer Fields: layer templates\n",
    "\n",
    "***Layer templates*** are declared using the following syntax:\n",
    "\n",
    "```\n",
    "<name>: fdl.Config[<type>] = template_field(<factory>)\n",
    "```\n",
    "\n",
    "Where `<factory>` is typically a Layer class name (e.g., `Bias`), but can also be a factory function that returns a Layer.  Layer templates are used to create child layers, by calling `self.create_child` from the `setup` method.\n",
    "\n",
    "### setup\n",
    "\n",
    "`setup` declares the layer variables/weights and its sublayers using\n",
    "\n",
    "- `self.create_variable`\n",
    "- `self.create_child`\n",
    "\n",
    "### setup: creating variables\n",
    "\n",
    "For example, the following declares a trainable weight `w`. Note that Pax requires users to statically annotate the shape and dtype of the weight, in addition to the usual initializer method.\n",
    "```\n",
    "self.create_variable(\n",
    "    'w',\n",
    "    WeightHParams(\n",
    "        shape=[self.input_dims, self.output_dims],\n",
    "        init=self.params_init,\n",
    "        dtype=self.dtype))\n",
    "```\n",
    "Trainable weights can be accessed as `self.theta.w`.\n",
    "\n",
    "The following declares a non-trainable weight `moving_mean`. `REQUIRES_MEAN_SYNC` tells the training loop to sync the mean of this variable after train step, which you can ignore for now.\n",
    "```\n",
    "mva = WeightHParams(\n",
    "    shape=[self.output_dims],\n",
    "    init=WeightInit.Constant(0.0),\n",
    "    dtype=self.dtype,\n",
    "    collections=[base_layer.WeightHParamsCollection.REQUIRES_MEAN_SYNC])\n",
    "self.create_variable(\n",
    "  'moving_mean',\n",
    "  mva,\n",
    "  trainable=False)\n",
    "```\n",
    "\n",
    "Non-trainable weights can be accessed via `self.get_var('moving_mean')`.\n",
    "\n",
    "### setup: creating child layers from templates\n",
    "\n",
    "`create_child(<name>, <layer_tpl>)` creates a child layer named `self.<name>` based on the layer template `<layer_tpl>`.  Here's an example of how to create a sublayer `self.linear` from a layer template `linear_tpl`:\n",
    "\n",
    "```\n",
    "def setup(self):\n",
    "  linear_tpl = self.linear_tpl.clone()\n",
    "  linear_tpl.set(\n",
    "      input_dims=self.input_dims,\n",
    "      output_dims=self.output_dims)\n",
    "  self.create_child('linear', linear_tpl)\n",
    "```\n",
    "\n",
    "### \\_\\_call\\_\\_\n",
    "\n",
    "`__call__` defines the forward-pass computation:\n",
    "- `self.theta.w` refers to trainable weight `w`\n",
    "- `self.get_var('moving_mean')` refers to non-trainable weight `moving_mean`\n",
    "- Trainable weights are immutable in `__call__` while non-trainable weights can be updated with `self.update_var('moving_mean', new_moving_mean)`.\n",
    "- Sublayer `__call__` can be expressed as\n",
    "  `projected_inputs = self.linear(inputs)`\n",
    "\n",
    "### randomness\n",
    "\n",
    "Often, users want to access randomness in `__call__`. All `jax.random.*` functions take a `jax.random.PRNGKey(some_int)` as an argument.\n",
    "\n",
    "**Important:** The same key always maps to the same random bits. (On the first order, users can think of JAX random function as a deterministic hash from a key to some random bits.) Because of this, users must always use a new random key per invocation.\n",
    "\n",
    "Typically, to avoid key re-use, JAX users are taught to split the key with `key, subkey=jax.random.split(key)` and use `subkey`, but for convenience, Pax BaseLayer handles the key splitting internally and provides `self.next_prng_key()` that gives users a new key on every invocation.\n",
    "\n",
    "See https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html?highlight=random# for more on JAX randomness.\n",
    "\n",
    "An example of `self.next_prng_key` in action:\n",
    "\n",
    "```\n",
    "\n",
    "def __call__(self, inputs):\n",
    "  prng_key = self.next_prng_key()\n",
    "  random_tensor = jax.random.uniform(\n",
    "      prng_key, inputs.shape, dtype=inputs.dtype)\n",
    "  inputs_with_noise = inputs + random_tensor\n",
    "  ...\n",
    "```\n",
    "\n",
    "### summaries\n",
    "\n",
    "Users may want to report summaries to be shown in TensorBoard. Inside layer `__call__`, users can do so with `self.add_summary`. Example:\n",
    "\n",
    "```\n",
    "self.add_summary(\n",
    "  'inputs_mean', jnp.mean(inputs))\n",
    "```\n",
    "\n",
    "### Note to Lingvo users\n",
    "\n",
    "For Lingvo users:\n",
    "\n",
    "  - No more explicit `theta` argument to `fprop`\n",
    "  - Same APIs for creating variables and sublayers:\n",
    "    - `self.create_variable`\n",
    "    - `self.create_child`\n",
    "  - Slightly different ways to add summaries and aux_loss\n",
    "    - `self.add_summary`\n",
    "    - `self.add_aux_loss`\n",
    "\n",
    "Let's put all those in action in a few layer definitions. The layer `__call__` logic is for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBwndn6sXUaX"
   },
   "outputs": [],
   "source": [
    "class Linear(base_layer.BaseLayer):\n",
    "  \"\"\"A simple linear layer.\n",
    "\n",
    "  Attributes:\n",
    "    input_dims: Depth of the input.\n",
    "    output_dims: Depth of the output.\n",
    "  \"\"\"\n",
    "  input_dims: int = 0\n",
    "  output_dims: int = 0\n",
    "\n",
    "  def setup(self):\n",
    "    # create_variable creates trainable variable, similar to Flax' self.param.\n",
    "    self.create_variable(\n",
    "        'w',\n",
    "        WeightHParams(\n",
    "            shape=[self.input_dims, self.output_dims],\n",
    "            init=self.params_init,\n",
    "            dtype=self.dtype))\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    # Use self.theta.\n",
    "    return jnp.matmul(inputs, self.theta.w)\n",
    "\n",
    "\n",
    "class Bias(base_layer.BaseLayer):\n",
    "  \"\"\"A simple bias layer.\n",
    "\n",
    "  Attributes:\n",
    "    dims: Depth of the input.\n",
    "  \"\"\"\n",
    "  dims: int = 0\n",
    "\n",
    "  def setup(self):\n",
    "    self.create_variable(\n",
    "        'b',\n",
    "        WeightHParams(\n",
    "            shape=[self.dims],\n",
    "            init=WeightInit.Constant(0.0),\n",
    "            dtype=self.dtype))\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    return inputs + self.theta.b\n",
    "\n",
    "\n",
    "class FeedForward(base_layer.BaseLayer):\n",
    "  \"\"\"A basic feed-forward layer.\n",
    "\n",
    "  Attributes:\n",
    "    input_dims: Depth of the input.\n",
    "    output_dims: Depth of the output.\n",
    "    has_bias: Adds bias weights or not.\n",
    "    linear_tpl: Linear layer params\n",
    "    activation_tpl: Activation function to use.\n",
    "  \"\"\"\n",
    "  input_dims: int = 0\n",
    "  output_dims: int = 0\n",
    "  has_bias: bool = True\n",
    "  linear_tpl: LayerTpl = template_field(Linear)\n",
    "  activation_tpl: pax_fiddle.Config[activations.BaseActivation] = template_field(\n",
    "      activations.ReLU)\n",
    "\n",
    "  def setup(self):\n",
    "    linear_tpl = self.linear_tpl.clone()\n",
    "    linear_tpl.input_dims=self.input_dims\n",
    "    linear_tpl.output_dims=self.output_dims\n",
    "    # Provide type hint.\n",
    "    self.linear: Linear\n",
    "    self.create_child('linear', linear_tpl)\n",
    "\n",
    "    if self.has_bias:\n",
    "      bias_layer_tpl = pax_fiddle.Config(Bias, dims=self.output_dims)\n",
    "      # Provide type hint.\n",
    "      self.bias: Bias\n",
    "      self.create_child('bias', bias_layer_tpl)\n",
    "\n",
    "    # Provide type hints\n",
    "    self.activation: activations.BaseActivation\n",
    "    self.create_child('activation', self.activation_tpl)\n",
    "\n",
    "    # To demonstrate how to add a non-trainable var e.g. batch stats.\n",
    "    # Set trainable=False\n",
    "    mva = WeightHParams(\n",
    "        shape=[self.output_dims],\n",
    "        init=WeightInit.Constant(0.0),\n",
    "        dtype=self.dtype,\n",
    "        collections=[base_layer.WeightHParamsCollection.REQUIRES_MEAN_SYNC])\n",
    "    self.create_variable(\n",
    "      'moving_mean',\n",
    "      mva,\n",
    "      trainable=False)\n",
    "\n",
    "  # For illustration purposes, we demo how to add summary, get randomness,\n",
    "  # add aux loss and update non-trainable vars. You wouldn't normally do this\n",
    "  # for a FeedForward layer.\n",
    "  def __call__(self, inputs):\n",
    "    # Add a summary to the `summaries` variable collection.\n",
    "    self.add_summary(\n",
    "      'inputs_mean', jnp.mean(inputs))\n",
    "\n",
    "    # Demonstrate how to get randomness with self.next_prng_key.\n",
    "    prng_key = self.next_prng_key()\n",
    "    random_tensor = jax.random.uniform(\n",
    "        prng_key, inputs.shape, dtype=inputs.dtype)\n",
    "    inputs_with_noise = inputs + random_tensor\n",
    "\n",
    "    # No longer need to provide self.theta.linear.\n",
    "    projected_inputs = self.linear(inputs_with_noise)\n",
    "    if self.hparams.has_bias:\n",
    "      projected_inputs = self.bias(projected_inputs)\n",
    "    output = self.activation(projected_inputs)\n",
    "\n",
    "    # Add a dummy aux_loss.\n",
    "    self.add_aux_loss('dummy_aux_loss', jnp.mean(output))\n",
    "\n",
    "    # Read and update non-trainable var.\n",
    "    old_v = self.get_var('moving_mean')\n",
    "    new_v = old_v + 1.0\n",
    "    self.update_var('moving_mean', new_v)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXXRo83bwT3U"
   },
   "source": [
    "## Instantiate a layer and introspect how variable collections are tracked\n",
    "\n",
    "It is useful to use Colab to interactively play around with the example layers to get a mental model of Pax and its relation to Flax nn.Module. The following sections should be read after getting some familarity with Flax nn.Module APIs to appreciate the similarity and differences with PAX. See https://flax.readthedocs.io/en/latest/index.html.\n",
    "\n",
    "- a `fdl.Config[Layer]` object `p` can be instantiated via `instantiate(p)` to get an object of Layer itself.\n",
    "- Similar to Flax nn.Module APIs, Pax layers can be initialized and run with `layer.init` and `layer.__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEEQq7sOr2zr"
   },
   "outputs": [],
   "source": [
    "# Create a new layer using Fiddle.\n",
    "ffn_cfg = pax_fiddle.Config(\n",
    "    FeedForward,\n",
    "    name='ffn', input_dims=1, output_dims=2)\n",
    "ffn: FeedForward = instantiate(ffn_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WERRv0kLxTvg"
   },
   "source": [
    "### initial_vars\n",
    "\n",
    "`ffn.init` takes a RNG key for variable initialization. The returned `initial_vars` is a nested dict of different variable collections that are initialized:\n",
    "\n",
    "- 'params' collection includes the trainable variables\n",
    "- 'non_trainable' collections includes non-trainable ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JZT-ywLwS9k"
   },
   "outputs": [],
   "source": [
    "npy_inputs = np.random.normal(1.0, 0.5, [2, 2, ffn_cfg.input_dims]).astype(np.float32)\n",
    "inputs = jnp.asarray(npy_inputs)\n",
    "\n",
    "prng_key = jax.random.PRNGKey(seed=123)\n",
    "prng_key, init_key, random_key, noise_key = jax.random.split(prng_key, 4)\n",
    "# Similar to Flax, use layer.init to initialize the layer and return initial_vars.\n",
    "# Note Pax doesn't support shape inference.\n",
    "initial_vars = ffn.init({PARAMS: init_key, RANDOM: random_key}, inputs)\n",
    "pprint(initial_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZKKnXWPyuHW"
   },
   "source": [
    "### layer.apply\n",
    "\n",
    "`layer.apply` returns the outputs without mutating any variable collections by default.\n",
    "\n",
    "- Provide `rngs={RANDOM: noise_key}` to pass in RNG key stream named RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qM7KMVZIuo3M"
   },
   "outputs": [],
   "source": [
    "outputs = ffn.apply(initial_vars, inputs, method=ffn.__call__, rngs={RANDOM: noise_key})\n",
    "pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5LnOOYey2sC"
   },
   "source": [
    "### layer.apply(..., mutable=True)\n",
    "`layer.apply(..., mutable=True)` returns the outputs with updated variable collections.\n",
    "- Set `mutable=True` to see `updated_vars`\n",
    "- Note that `summaries`, `aux_loss` are passed around just like `params` and `non_trainable` as separate Flax variable collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NW4juLftx2qC"
   },
   "outputs": [],
   "source": [
    "# Settig mutable=True will return both the outputs and the updated variable collections.\n",
    "outputs, updated_vars = ffn.apply(initial_vars, inputs, method=ffn.__call__, rngs={RANDOM: noise_key}, mutable=True)\n",
    "pprint(outputs)"
   ]
  }
 ],
 "metadata": {
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
